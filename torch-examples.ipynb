{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06374192\n"
     ]
    }
   ],
   "source": [
    "#use %%time to record the time taken for the cell to execute\n",
    "start_time = time.time()\n",
    "#matrix operations here\n",
    "zeros = torch.zeros(1,1)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"{elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.07070255\n",
      "1.27283192\n"
     ]
    }
   ],
   "source": [
    "#Generates 2 random matrices using pytorch and then numpy\n",
    "#Measures time taken to multiply by matrices\n",
    "#This is so we understand which library to use for the LLM\n",
    "torch_rand1 = torch.rand(10000, 10000).to(device)\n",
    "torch_rand2 = torch.rand(10000, 10000).to(device)\n",
    "np_rand1 = torch.rand(10000, 10000)\n",
    "np_rand2 = torch.rand(10000, 10000)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"{elapsed_time:.8f}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"{elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# torch.stack, torch.multinomial, torch.tril, torch.triu, input.T / input.transpose, nn.linear, torch.cat, F.softmax\n",
    "\n",
    "#Define a probability tensor\n",
    "probabilities = torch.tensor([0.1, 0.9])\n",
    "#10% or 0.1 that we get a 0, 90% or 0.9 chance that we get a 1. each probability points to the index of the probability in the tensor\n",
    "#Draw 5 samples from the multinomial distribution\n",
    "samples = torch.multinomial(probabilities, num_samples=10, replacement=True)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creates a tensor with values [1, 2, 3, 4]\n",
    "tensor = torch.tensor([1,2,3,4])\n",
    "#concatenates the tensor [1, 2, 3, 4] with the tensor [5] along the 0th dimension\n",
    "out = torch.cat((tensor, torch.tensor([5])), dim= 0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates a matrix of 1s only, then returns the lower triangular part \n",
    "#tril = triangle lower\n",
    "#1 models predicted events, and 0 models events that havent been predicted yet\n",
    "out = torch.tril(torch.ones(5,5))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same as above but triangle upper\n",
    "out = torch.triu(torch.ones(5,5))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exponentiate (2.71) every element, e.g. 2.71^0 = 1, 2..71^-inf = 0\n",
    "out = torch.zeros(5, 5).masked_fill(torch.tril(torch.ones(5,5)) == 0, float('-inf'))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.zeros(2,3,4)\n",
    "#Swap dimensions with transpose()\n",
    "out = input.transpose(0,2)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor([1,2,3])\n",
    "tensor2 = torch.tensor([4,5,6])\n",
    "tensor3 = torch.tensor([7,8,9])\n",
    "\n",
    "#Stack tensors along a new dimension to make a batch\n",
    "stacked_tensor = torch.stack([tensor1, tensor2, tensor3])\n",
    "stacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.6842,  3.2855,  0.1867], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "#nn (neural network) module contains learnable parameters\n",
    "#applies a linear transformation to the incoming data, which is defined as y = xA^T + b\n",
    "#where x is the input tensor, A is the weight tensor, b is the bias tensor, and y is the output tensor\n",
    "import torch.nn as nn\n",
    "sample = torch.tensor([10.,10.,10.])\n",
    "linear = nn.Linear(3,3, bias=False)\n",
    "print(linear(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "#Create a tensor\n",
    "tensor1 = torch.tensor([1.0,2.0,3.0])\n",
    "\n",
    "#Apply softmax using torch.nn.functional.softmax()\n",
    "#softmax(x_i) = exp(x_i) / sum(exp(x_j))\n",
    "# exponentiate the input vector and then divide by the sum of the exponentiated values to normalize the output\n",
    "#so that it can be interpreted as a probability distribution\n",
    "softmax_output = F.softmax(tensor1, dim=0)\n",
    "\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "tensor([[-0.4971,  0.4729,  0.0194, -0.8119, -1.6965,  0.6117],\n",
      "        [-2.3138, -0.2311, -1.0784, -0.8175,  1.1843,  1.1019],\n",
      "        [ 0.5366, -0.4469, -0.0517,  2.8779, -0.7432, -0.4882],\n",
      "        [-0.0876, -0.0231, -0.2826, -0.1637, -0.5108,  0.3409]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Initialise an embedding layer\n",
    "#vocab_size is the size of the dictionary of embeddings (number of rows)\n",
    "vocab_size = 80\n",
    "#embedding_dim is the size of each embedding vector (number of columns)\n",
    "embedding_dim = 6\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "#Create some input indices\n",
    "#list of indices that correspond to the words in your vocabulary\n",
    "input_indices = torch.LongTensor([1, 5, 3, 2])\n",
    "\n",
    "#Apply the embedding layer\n",
    "embedded_output = embedding(input_indices)\n",
    "\n",
    "#The output will be a tensor of shape (4, 100), where 4 is number of inputs\n",
    "#and 100 is dimensionality of embedding vectors\n",
    "#Each row in the output tensor represents an embedding vector for a word in your vocabulary\n",
    "print(embedded_output.shape)\n",
    "print(embedded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27,  30,  33],\n",
      "        [ 61,  68,  75],\n",
      "        [ 95, 106, 117]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],[3,4],[5,6]])\n",
    "b = torch.tensor([[7,8,9],[10,11,12]])\n",
    "# print(a @ b)\n",
    "print(torch.matmul(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "int_64 = torch.randint(1, (3, 2)).float()\n",
    "#type int64\n",
    "float_32 = torch.rand(2,3)\n",
    "#type float32\n",
    "# print(int_64.dtype, float_32.dtype)\n",
    "result = torch.matmul(int_64, float_32)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "#creates a tensor of shape (2, 3, 5) with random values between 0 and 1\n",
    "a = torch.rand(2, 3, 5)\n",
    "print(a.shape)\n",
    "#unpacks the shape of the tensor into three variables\n",
    "x, y, z = a.shape\n",
    "a = a.view(x,y,z)\n",
    "# print(x, y, z)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4394, 0.1717, 0.7571, 0.6782, 0.2371, 0.0914, 0.9774, 0.7899, 0.1303,\n",
      "         0.3030],\n",
      "        [0.2766, 0.7958, 0.8020, 0.6955, 0.6326, 0.4468, 0.1796, 0.9522, 0.7304,\n",
      "         0.7200],\n",
      "        [0.7972, 0.2609, 0.2898, 0.8218, 0.3028, 0.2769, 0.6641, 0.0464, 0.1306,\n",
      "         0.8955],\n",
      "        [0.3694, 0.3220, 0.2227, 0.5814, 0.7105, 0.0363, 0.3237, 0.4678, 0.1850,\n",
      "         0.7279],\n",
      "        [0.0757, 0.3911, 0.0380, 0.0210, 0.0857, 0.5724, 0.5446, 0.5311, 0.8942,\n",
      "         0.9466],\n",
      "        [0.3389, 0.6812, 0.6447, 0.3236, 0.1608, 0.4118, 0.9511, 0.8297, 0.0216,\n",
      "         0.5838],\n",
      "        [0.1455, 0.7162, 0.4766, 0.5543, 0.9786, 0.5666, 0.6339, 0.7358, 0.3760,\n",
      "         0.1205],\n",
      "        [0.4084, 0.8237, 0.9453, 0.7386, 0.2686, 0.1598, 0.5625, 0.5251, 0.5881,\n",
      "         0.5821],\n",
      "        [0.0825, 0.8532, 0.1607, 0.7779, 0.5082, 0.7726, 0.0669, 0.4325, 0.9921,\n",
      "         0.7020],\n",
      "        [0.1126, 0.6090, 0.6649, 0.3436, 0.1271, 0.6355, 0.7352, 0.2743, 0.8078,\n",
      "         0.9145],\n",
      "        [0.8500, 0.0325, 0.7878, 0.5316, 0.9561, 0.4522, 0.0155, 0.8670, 0.5637,\n",
      "         0.9646],\n",
      "        [0.7271, 0.3227, 0.9567, 0.0144, 0.4588, 0.1894, 0.6465, 0.9315, 0.8647,\n",
      "         0.8401],\n",
      "        [0.1511, 0.2790, 0.9873, 0.0592, 0.3140, 0.7681, 0.8591, 0.0980, 0.0725,\n",
      "         0.9356],\n",
      "        [0.4230, 0.8103, 0.1747, 0.4596, 0.0559, 0.6131, 0.8180, 0.0430, 0.3681,\n",
      "         0.9841],\n",
      "        [0.2736, 0.9734, 0.9202, 0.3618, 0.6790, 0.3040, 0.5872, 0.3822, 0.9916,\n",
      "         0.9711],\n",
      "        [0.1013, 0.8613, 0.8131, 0.3814, 0.4692, 0.3763, 0.4271, 0.7898, 0.6782,\n",
      "         0.4151],\n",
      "        [0.2751, 0.2749, 0.5187, 0.8295, 0.1410, 0.8121, 0.7760, 0.5738, 0.2830,\n",
      "         0.6512],\n",
      "        [0.1598, 0.1339, 0.5818, 0.4159, 0.3939, 0.0899, 0.3184, 0.0287, 0.1844,\n",
      "         0.4751],\n",
      "        [0.3498, 0.9130, 0.6757, 0.5678, 0.3664, 0.6839, 0.8792, 0.5101, 0.2264,\n",
      "         0.9808],\n",
      "        [0.5138, 0.2296, 0.5284, 0.9409, 0.5093, 0.5250, 0.7773, 0.3224, 0.6128,\n",
      "         0.2802],\n",
      "        [0.4697, 0.5589, 0.8925, 0.9359, 0.8591, 0.0652, 0.5589, 0.6278, 0.9835,\n",
      "         0.0383],\n",
      "        [0.2710, 0.0092, 0.6368, 0.6598, 0.5202, 0.3765, 0.0680, 0.9873, 0.8309,\n",
      "         0.0951],\n",
      "        [0.2874, 0.7449, 0.1439, 0.7561, 0.9497, 0.0553, 0.8955, 0.5214, 0.9321,\n",
      "         0.4526],\n",
      "        [0.8484, 0.4864, 0.6915, 0.3270, 0.3006, 0.4445, 0.8336, 0.6075, 0.9567,\n",
      "         0.2343],\n",
      "        [0.2286, 0.6960, 0.6269, 0.4894, 0.7304, 0.2126, 0.6169, 0.1767, 0.7350,\n",
      "         0.1555],\n",
      "        [0.0282, 0.9235, 0.1803, 0.8892, 0.5933, 0.1736, 0.7435, 0.7494, 0.4252,\n",
      "         0.6736],\n",
      "        [0.4855, 0.7709, 0.4264, 0.3956, 0.3974, 0.9203, 0.9863, 0.3541, 0.1243,\n",
      "         0.1589],\n",
      "        [0.7542, 0.2240, 0.5717, 0.2423, 0.7571, 0.5261, 0.2113, 0.7534, 0.4362,\n",
      "         0.5245],\n",
      "        [0.3352, 0.9387, 0.2775, 0.8662, 0.6052, 0.5228, 0.0139, 0.5419, 0.6072,\n",
      "         0.3339],\n",
      "        [0.8387, 0.2462, 0.0113, 0.6166, 0.8196, 0.6599, 0.0397, 0.1650, 0.8934,\n",
      "         0.8974],\n",
      "        [0.4448, 0.3493, 0.9237, 0.9372, 0.2551, 0.9470, 0.6179, 0.6890, 0.7973,\n",
      "         0.5730],\n",
      "        [0.6484, 0.4207, 0.7657, 0.3336, 0.1307, 0.3577, 0.0993, 0.5766, 0.5456,\n",
      "         0.3594]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(input)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "input = torch.rand((4, 8, 10))\n",
    "B, T, C = input.shape\n",
    "output = input.view(B*T, C)\n",
    "print(output)\n",
    "# print(input)\n",
    "print(output[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([10], dtype=torch.float32)\n",
    "y = F.tanh(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation functions\n",
    "#ReLu works in forward passes - if a number is below 0, number will be turned into 0\n",
    "#Sigmoid maps any input value to a value between 0 and 1- 1/(1+ exp(-x)) = 2.71^-x\n",
    "#Tanh = hyperboliic tangent, outputs values between -1 and 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
