{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 1000\n",
    "eval_interval = 2500\n",
    "eval_iters = 250\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokeniser\n",
    "#implementation of a simple substitution cipher (for character in string, for integer in list)\n",
    "#enumerate() function is used to return both the index and the character of each element in the chars string\n",
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[73, 61, 58, 66,  1, 65, 68, 68],\n",
      "        [54, 71, 57,  0, 68, 59, 73, 58],\n",
      "        [58, 54, 71, 65, 78,  1, 54, 72],\n",
      "        [65, 58,  1, 54, 67, 57,  1, 73]])\n",
      "targets: \n",
      "tensor([[61, 58, 66,  1, 65, 68, 68, 64],\n",
      "        [71, 57,  0, 68, 59, 73, 58, 67],\n",
      "        [54, 71, 65, 78,  1, 54, 72,  1],\n",
      "        [58,  1, 54, 67, 57,  1, 73, 61]])\n"
     ]
    }
   ],
   "source": [
    "#n is an integer that represents 80% of the length of the input data. \n",
    "#train_data is a slice of the input data from the beginning to index n\n",
    "#while val_data is a slice from index n to the end.\n",
    "n = int(0.8 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    dat = train_data if split =='train' else val_data\n",
    "    #Random index ix\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    #print(ix)\n",
    "    #Offsetting by 1 as indexing starts at 0\n",
    "    #Stacks in batches\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "x, y = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(x)\n",
    "print('targets: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensures PyTorch doesn't use gradients, as that would reduce computation/ memory usage\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out= {}\n",
    "    #Sets to evaluation mode\n",
    "    model.eval()\n",
    "#Returns a dictionary containing the mean loss values for the training and validation sets\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        #iterates over the training and validation sets and computes the loss for each iteration\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    #Switching to training mode\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([80]) target is tensor(1)\n",
      "When input is tensor([80,  1]) target is tensor(1)\n",
      "When input is tensor([80,  1,  1]) target is tensor(28)\n",
      "When input is tensor([80,  1,  1, 28]) target is tensor(39)\n",
      "When input is tensor([80,  1,  1, 28, 39]) target is tensor(42)\n",
      "When input is tensor([80,  1,  1, 28, 39, 42]) target is tensor(39)\n",
      "When input is tensor([80,  1,  1, 28, 39, 42, 39]) target is tensor(44)\n",
      "When input is tensor([80,  1,  1, 28, 39, 42, 39, 44]) target is tensor(32)\n"
     ]
    }
   ],
   "source": [
    "#Given character \"context\", we can predict the next character should be \"target\"\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(\"When input is\", context, \"target is\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SrWJaedjsnXjU]LTu1DWx&iY*J7iH!Vi'&KwfmpAw19ioF0p;UUv[J[r G',)h)XicOI5Igvmh!vmm*tx\" fHT4A.9]( (_[6[;fVPTm9﻿I;9 ROTL6hjP)(ng(OrkUz:WTkd﻿nQEl5l?]EQ\n",
      "AO5M6,Arn383vbNvr﻿jSk-msq0Q:whl7zl,gS﻿3icS!caGAof?]J*;k]TvHypgUw9Zi﻿)fLv*\n",
      "Gslvc?nFD_T0BZnIcdtOG*J]ZkHfmhNI[lCic89peFJ﻿&4iw9Z6:WR.;.OIjVj!yc,&R'V;Ubd3IgOJ[)lSppR_)eo]Jl5Nk)''A﻿U'D﻿G-k;U[_Lc'7(-X]1-S(PIrHn_XMn6i)ma&﻿0_T4Rb D﻿IqDTo1ZAdVo18'3WH)Gs?HI.dM,0p:Aoihi.3S,UrSwN?L dG-SeSvxla8'Sa1lvE!a_[qP[40Nr5as5M﻿'Bbd!C0kvHCiuZZgOoFRoo]F?hVanIM5gPcPmge2K5IsUAd\"dU\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        #logits are a probability distribution of what we want to predict\n",
    "        #3 dimensional\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #B is batch, T is time dimension, C is channels (vocab size)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            #PyTorch excepts a certain shape (Batch size, Channels), thats why we reshape it \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        #index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #Get predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            #Focus only on the last time step\n",
    "            logits = logits[:, -1, :] #Becomes (B, C)\n",
    "            #Apply softmax to get probabilities, focusing on the last dimension\n",
    "            probs = F.softmax(logits, dim=-1) #(B, C)\n",
    "            #Sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) #(B, 1)\n",
    "            #Append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) #(B, T+1)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "#Push parameters to GPU for more efficient training \n",
    "m = model.to(device)\n",
    "\n",
    "#torch.long = int64\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.8349, val loss: 3.8287\n",
      "step 250: train loss 3.8058, val loss: 3.7826\n",
      "step 500: train loss 3.7493, val loss: 3.7472\n",
      "step 750: train loss 3.6907, val loss: 3.6873\n",
      "3.6330301761627197\n"
     ]
    }
   ],
   "source": [
    "#Create a PyTorch optimiser\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#iter = iterations\n",
    "for iter in range(max_iters):\n",
    "    #Check if remainder of current iteration/ eval_iters is 0\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
    "        \n",
    "    #Sample a batch of data\n",
    "    #x inputs and y targets\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    #Evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    #Backward pass\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Srbk(Ag1qDM(3F-]T0Z!a 6HrWV!KZ*]LsDudkG'FOIf\n",
      ")a8b?hlwVv(T\n",
      ")A[KJpipl)Bt-HsXKOpJI!﻿S﻿YtBg_gVo])xfmi]cRouFFQX fd wWR0!M6i6]2j:jk7nQCZd0,6a-7[q7xm4&qHs?iX&0l&?QcasC'IM6V'i])w]l57iYgG':9ODF8F!aVJBDL?usvHAg]jC;lPBBtj;*;UQ5;I-_mn)'G'S(QB:J,ZwWzlpY:UYf-2F?dK8B.so7)gu[﻿&B6:!Vl!NS)(6[vyCFzZu1x9ls!NnJwsu]-2Va\"bnw.wKCOM dT0Z4.BZLkp9_by: lO\n",
      "FBgcbNM(AG]﻿jV'WC siZh;U﻿nQ8\n",
      "hCr﻿5MqxpG*_FJqxoHCE2\"Z7aH!eS(\"ZZ83\n",
      "Xp;.VmyAotAMJ?d3LUvca-BF-4M8e5DN\"]!viXaU!U!VoegR)i]xEa,U?HC7hVQVa_bsTh;D6gv3o,VoYNF3 l,V AFONyy_0UjVI1YH)\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMISERS\n",
    "# Mean Squared Error (MSE): MSE is a common loss function used in regression problems, where the goal is to predict a continuous output. It measures the average squared difference between the predicted and actual values, and is often used to train neural networks for regression tasks.\n",
    "# Gradient Descent (GD): is an optimization algorithm used to minimize the loss function of a machine learning model. The loss function measures how well the model is able to predict the target variable based on the input features. The idea of GD is to iteratively adjust the model parameters in the direction of the steepest descent of the loss function\n",
    "# Momentum: Momentum is an extension of SGD that adds a \"momentum\" term to the parameter updates. This term helps smooth out the updates and allows the optimizer to continue moving in the right direction, even if the gradient changes direction or varies in magnitude. Momentum is particularly useful for training deep neural networks.\n",
    "# RMSprop: RMSprop is an optimization algorithm that uses a moving average of the squared gradient to adapt the learning rate of each parameter. This helps to avoid oscillations in the parameter updates and can improve convergence in some cases.\n",
    "# Adam: Adam is a popular optimization algorithm that combines the ideas of momentum and RMSprop. It uses a moving average of both the gradient and its squared value to adapt the learning rate of each parameter. Adam is often used as a default optimizer for deep learning models.\n",
    "# AdamW: AdamW is a modification of the Adam optimizer that adds weight decay to the parameter updates. This helps to regularize the model and can improve generalization performance. We will be using the AdamW optimizer as it best suits the properties of the model we will train in this video."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
